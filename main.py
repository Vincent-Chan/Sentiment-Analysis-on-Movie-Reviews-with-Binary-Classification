# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_C226SLQgCFKwonFfsagMmPOJB3JYrof
"""

!ls -lha

from google.colab import drive
drive.mount('/content/drive')

# Import the necessary libraries

import numpy as np
import pandas as pd
import re
import nltk
import pickle
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
from nltk.stem import WordNetLemmatizer
from sklearn import metrics
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# read the csv file (i.e. the dataset)

data = pd.read_csv('/content/drive/MyDrive/Sentiment Analysis on Movie Reviews with Binary Classification/IMDB-Dataset.csv')

print("Shape:", data.shape)

# Show the first 10 rows of dataset

data.head(10)

# Show the brief information of the dataset

data.info()

# Show the statistical summary of the data

data.describe()

# Count the number of positive comments & negative comments respectively

data.sentiment.value_counts()

# Replace the positive comments with the label "1"
# Replace the negative comments with the label "0"

data.sentiment.replace('positive', 1, inplace=True)
data.sentiment.replace('negative', 0, inplace=True)

# Show the first 10 rows of data after processing

data.head(10)

# Show the 101th comments

data.review[100]

"""**Now we are goint to do some preprocessing on the dataset**

1. Remove HTML tags
2. Remove special characters and symbols
3. Convert all characters to lowercase  
4. Remove stopwords
5. Stemming/Lemmatization (Choose 1 out of 2)

*(Note: Usually we do stemming as stemming is faster than lemmatization, but lemmatization is more accurate in processing the words, so we give the user to choose which one they want to do.)*

***By default, I use Stemming since it is faster in processing than lemmatization***
"""

# 1. Remove HTML tags

def clean(text):
    cleaned = re.compile(r'<.*?>')
    return re.sub(cleaned,'',text)

data.review = data.review.apply(clean)
data.review[100]

# 2. Remove special characters and symbols

def is_special(text):
    rem = ''
    for i in text:
        if i.isalnum():
            rem = rem + i
        else:
            rem = rem + ' '
    return rem

data.review = data.review.apply(is_special)
data.review[100]

# 3. Convert all characters to lowercase

def to_lower(text):
    return text.lower()

data.review = data.review.apply(to_lower)
data.review[100]

# 4. Remove stopwords

def rem_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    return [w for w in words if w not in stop_words]

data.review = data.review.apply(rem_stopwords)
data.review[100]

# 5(a). Stemming

# Let's use stemming as default

def stem_txt(text):
    ss = SnowballStemmer('english')
    return " ".join([ss.stem(w) for w in text])

data.review = data.review.apply(stem_txt)
data.review[100]

data.head(10)

# 5(b). Lemmatization

def lemma_txt(text):
    wnl = WordNetLemmatizer()
    return " ".join([wnl.lemmatize(w) for w in text])

"""Now, we build the model

1. Creating bag of words (BOW)
2. Train test split
3. Defining the models and Training them
4. Prediction and accuracy metrics to choose best model
"""

# 1. Create bag of words

X = np.array(data.iloc[:,0].values)
y = np.array(data.sentiment.values)
cv = CountVectorizer(max_features = 1000)
X = cv.fit_transform(data.review).toarray()

print("X.shape = ", X.shape)
print("y.shape = ", y.shape)

print(X)

print(y)

# 2. Train test split

trainx, testx, trainy, testy = train_test_split(X, y, test_size = 0.2, random_state = 9)
print("Train shapes : X = {}, y = {}".format(trainx.shape, trainy.shape))
print("Test shapes : X = {}, y = {}".format(testx.shape, testy.shape))

# 3. Defining the models and Training them

gnb, mnb, bnb = GaussianNB(), MultinomialNB(alpha=1.0,fit_prior=True), BernoulliNB(alpha=1.0,fit_prior=True)

gnb.fit(trainx, trainy)
mnb.fit(trainx, trainy)
bnb.fit(trainx, trainy)

# 4. Prediction and accuracy metrics to choose best model

ypg = gnb.predict(testx)
ypm = mnb.predict(testx)
ypb = bnb.predict(testx)

print("Gaussian = ",accuracy_score(testy,ypg))
print("Multinomial = ",accuracy_score(testy,ypm))
print("Bernoulli = ",accuracy_score(testy,ypb))

pickle.dump(bnb, open('model1.pkl','wb'))

choices = 0

while True:
    input_review = input("Please enter your review (enter e if you want to exit): ")

    if input_review == 'e':
        print("Program terminates")
        break

    choices = int(input("Enter 0 if you want to do stemming, enter 1 if you want to do lemmatization: "))

    f1 = clean(input_review)
    f2 = is_special(f1)
    f3 = to_lower(f2)
    f4 = rem_stopwords(f3)

    if choices == 0:
        f5 = stem_txt(f4)
    else:
        f5 = lemma_txt(f4)


    bow, words = [], word_tokenize(f5)
    for word in words:
        bow.append(words.count(word))

    word_dict = cv.vocabulary_
    pickle.dump(word_dict, open('bow.pkl', 'wb'))

    inp = []
    for i in word_dict:
        inp.append(f5.count(i[0]))
    y_pred = bnb.predict(np.array(inp).reshape(1,1000))

    if y_pred == 0:
        print("This is a negative review")
    else:
        print("This is a positive review")